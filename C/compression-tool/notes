The early founders of compression were not thinking about data. They were thinking about statistics. They were looking for, and found, different ways to manipulate the probablity distributions of symbols in data sets so that they could produce smaller data sets with the same meaning.

Compressions is about the most compact represntation of data.

Data compression algorithms fall into a few buckets; they are - variable length codes, statistical compression, dictionary encodings, context modeling, and multicontext modeling. Each of these five high level buckets contains a horde of algorithm variations, which is a good thing. Each variation differs slightly in intended input data, performance, memory constraints, and output sizes. Picking the correct variant means carrying out tests on your data and the encoders to find the one that works best.

Now you can use these buckets together, because some buckets contain algorithms whose entire purpose is to transform the data so that another bucket can be more efficient at compressing it.

You need to understand the buckets, how they fit together, and what types of variants to use from which bucket for your own data sets.

Messages can be encoded in many ways, think alphabet or morse code, but for every message, there is a most efficient way to encode it. Where efficient means using the fewest possible letters or symbols (or bits or units of information). What fewest boils down to depends on the information content of the message. Shannon invented a way of measuring the information content of a message and called it information entropy.

Data compression is a practical application of Shannon's research, which asks, "How compact can we make a message before we can no longer recover it?"

It's important to note that according to modern information theory, there is a point at which removing any more bits removes the ability for you to uniquely recover your data stream properly. So the goal of compression is to remove as many bits as possible to get to this point, and then remove no more.

The only thing you need to know about data compression; data compression works via two simple ideas:
1. reduce the number of unique symbols in your data (smallest possible "alphabet")
2. encode more frequent symbols with fewer bits (fewest bits for most common "letters")

What makes applied data compression so complex is that there are many ways to do these two things, depending on what kind of data you have. You will need to take the following into consideration:

1. different data will need to be treated differently. Words in a book and floating point numbers for example respond effectively to very different algorithms.
2. some data can be transformed first to make it more compressible
3. data might be skewed. for example, temperature data taken in summer might be skewed toward high temperatures.

Your challenge as a programmer is to figure out the best way, or combination of ways for compressing any block of data that a user throws at your application. And your challenge as a content developer is to figure out how to throw data at your users and not break their bank accounts.

Compression and the economy:
Compressed files are smaller files meaning it takes less time to transfer them, and it costs less to do so as well. Distributors pay less to distribute, and customer pay less to consume. In a modern world in which computing time is literally money, compression represents the most economically viable way to shortedn the gap between content distributors and consumers.

Everything in data compression is about reducing the number of bits used to represent a given data set.

information theory: the mathematical study of the coding of information in the form of sequences of symbols, impulses etc.. and how rapidly such information can be trasmitted e.g. through computer circuits or telecommunication channels.

According to information theory, the information content of a number is equal to the number of binary (yes/no) decisions that you need to make before you can uniquely identify that number in a set. I now understand this statement.

if given a number, it is not immediately obvious how many bits it will require without going through the process of binary conversion which is tedious. The following mathematical formulae makes it easy for us to figure this out

log₂(x) = -(log(x)/log(2)) = number of binary digits needed to represent a number

e.g. log₂(10) = 3.321 bits. Of course, you cannot represent 3.321 bits on modern computer hardware (there are no fractions for bits), so we are forced to round up to the next whole integer, using the ceiling function i.e.

log₂(x) = ceil(log(x)/log(2))

Now there is another problem, we are off by one bit for powers of two e.g.

log₂(2) = ceil(log(2)/log(2)) = 1

log₂(4) = ceil(log(4)/log(2)) = 2

the results are true from a mathematics perspective but fails in the fact that we need two and three bits to represent numbers 2 (10) and 4 (100) on our systems. As such, we add a slight skewing to our method to ensure that our log results are accurate for powers of two.

log₂(x) = ceil(log(x+1)/log(2))

so given any decimal integer number, we can easily determine the minimum number of bits needed to represent it in binary by calculating its log₂. Shannon defined this log₂ of a variable as its entropy, or rather the least number of bits required to represent that value.

the log₂ form of numbers is efficient but not practical for the way we build computer components. why? the issue lies between representing the number in the least bits possible, confussion on how to decode a binary string of numbers (without knowing their log₂ lengths), and performance in hardware execution. I don't fully inderstand the preceding statement.

Modern computers compromise by using fixed-length buckets of bits for numbers of different sizes. The fundamental bucket is one byte, which is made up of eight bits. And the integer buckets typically available in modern programming languages are a short with 16 bits, an integer with 32 bits, and a long with 64 bits. As such, our deci‐ mal number 10, converted to binary as 1010, would be a short and represented as 0000000000001010. This is a lot of wasted bits.

The point here is that the majority of algorithms we use in the development of modern applications all tend to use defined bit ranges rather than the LOG2 size. Which is basically the difference between information theory and implementation practicality. Any bit stream we have will always be rounded up to the next byte- aligned size in computer memory. This can get confusing: for example, when we’ve just saved 7 bits of data, our machine reports that our data still remains the same number of bytes long.

The goal in practical data compression is to get as close as possible to the theoretical limit of compressibility. That’s why, to learn and understand compression algorithms, moving forward with the rest of this book, we will only think in terms of LOG2.