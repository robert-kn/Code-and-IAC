{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 CourierNewPSMT;\f1\fnil\fcharset0 Menlo-Regular;\f2\fswiss\fcharset0 ArialMT;
\f3\fswiss\fcharset0 Helvetica;\f4\fswiss\fcharset0 Helvetica-Bold;\f5\fswiss\fcharset0 Arial-BoldMT;
}
{\colortbl;\red255\green255\blue255;\red13\green14\blue16;\red255\green255\blue255;\red255\green255\blue255;
\red193\green193\blue193;\red24\green24\blue24;\red38\green38\blue38;\red183\green111\blue179;\red89\green138\blue67;
\red202\green202\blue202;\red194\green126\blue101;\red70\green137\blue204;\red212\green214\blue154;\red140\green211\blue254;
\red167\green197\blue152;\red205\green173\blue106;}
{\*\expandedcolortbl;;\cssrgb\c5882\c6667\c7843;\cssrgb\c100000\c100000\c100000;\csgray\c100000;
\cssrgb\c80000\c80000\c80000;\cssrgb\c12157\c12157\c12157;\cssrgb\c20000\c20000\c20000;\cssrgb\c77255\c52549\c75294;\cssrgb\c41569\c60000\c33333;
\cssrgb\c83137\c83137\c83137;\cssrgb\c80784\c56863\c47059;\cssrgb\c33725\c61176\c83922;\cssrgb\c86275\c86275\c66667;\cssrgb\c61176\c86275\c99608;
\cssrgb\c70980\c80784\c65882;\cssrgb\c84314\c72941\c49020;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}}
\paperw11900\paperh16840\margl1440\margr1440\vieww33100\viewh16960\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs48 \cf0 What is the core purpose of data engineering? Working with accurate data to make to make it accessible and available when it is needed to make informed decisions. Data warehouses play a significant role in making this possible. what is a data warehouse? A large repository of data where data has been organised and cleaned for analysis and reporting.\
\
How is data engineering done? Architecting and managing the pipelines that collect, transform, integrate and store of data.\
\
What are the core concepts of data engineering (what data engineers need to know)? And What is the ecosystem of data engineering?\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic.png \width26480 \height13160 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
What is the lifecycle of data engineering? Access the data, process the data to conform to org standards, challenges in this phase include: data management (repositories that provide high availability, flexibility, accessibility and security), providing the interfaces, APIs, and applications that give access to users (data scientists/analysts, programmers, apps) who need access to the data\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic 1.png \width26480 \height12580 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Different data sources available: structured, semi-structured, and unstructured.\cf2 \cb3 \expnd0\expndtw0\kerning0
The type of data drives the kind of data repositories that the data can be collected and stored\'a0in, and also the tools that can be used to query or process the data.\'a0\
\
Different types of repositories: OLTP and OLAP\
\
What are data pipelines? A set of tools and processes that cover the entire journey from source to destination systems. The processes include ETL or ELT; tools used include programming languages, query languages and scripting languages.\
\
Difference between ETL and ELT and where they are applicable. Common pipeline tools used; apache beam, dataflow, and airflow (for workflow orchestration).\
\
What data integration is: delivering data through an integrated approach (data consolidation) for analytical purposes by transforming and merging extracted data.\
\
What is data? \cf0 \cb4 is unorganised information that is processed to make it meaningful. It is comprised of facts, observations, perceptions, characters, symbols, numbers, images.\
\
The different file formats that are used to store data processed by data engineers.\
\
The common sources of data encountered by data engineers in their quest to ingest data (e.g. databases, data streams, APIs and web services etc)\
\
Commonly used data streaming technologies: apache kafka, apache samza, apache storm.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1\fs24 \cf5 \cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs48 \cf0 \kerning1\expnd0\expndtw0 \
\
Emerging technologies that are shaping the modern data ecosystem? \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic 2.png \width20400 \height11140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic 3.png \width23520 \height11140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Key players in the data ecosystem? Data engineers, data analysts, data scientists, business analysts and BI analysts \
\
Technical skills needed to be a data engineer? Knowledge of operating systems, infra components (such VMs, Networking, cloud based services), working with databases (working with RDBMS, NoSQL) and data warehouses, working with data pipelines (apache beam, dataflow etc). ETL tools (proprietary and open source), programming languages, scripting languages, query languages, big data processing tools (Hadoop, HDFS, hive, and spark). \
\
Metadata and metadata management within the context of databases, data warehousing, business \
Intelligence systems. The different types of metadata (technical, process, business) and what they entail. Why metadata management is important.\
\
Overview of data repositories: come in 3 flavours (databases, data warehouses, and big data stores). Difference between databases and DBMS. Factors that govern choice of database (latency requirements, transactions speeds etc). Relational v non relational. Data warehouses used for consolidating data through the extract, transform and load process. what data marts are and how they are used. Big data stores being distributed computational and storage infrastructure to store, scale, and process very large data sets. What data lakes are and what they are used for.\
\
Foundations of big data: the 5 V\'92s of big data.\
\
Layers of a data platform architecture:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic 1__#$!@%!#__Pasted Graphic.png \width33320 \height12480 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Factors affecting the selection and designing of data stores: type of data (relational v non relational), volume of data (e.g. opt for bid data store if dealing with high volume high velocity, diverse data if it needs to be processed), privacy, security, and governance, intended use of the data (number of transactions, frequency of updates, type of operations, response time etc), performance (throughput, latency), availability, integrity, and recoverability of data.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \expnd0\expndtw0\kerning0
The CIA, or Confidentiality, Integrity, and Availability triad are three key components of an effective strategy for information security.
\f2\fs32  \
\

\f0\fs48 Data wrangling: some of the activities that are to be carried out to make data, analytics ready; data profiling, structuring data, normalisation, denormalising, and data visualisation (to identify outliers)\
\
Performance tuning and troubleshooting: monitor and optimise systems and data flows for performance and availability because a data pipeline typically runs with a combination of complex tools and can face several performance threats. These threats include scalability in the face of increasing data sets and workloads, application failures, tool incompatibilities, scheduled jobs not starting when they should.\
\
Governance and compliance: collection of principles, practices, and processes to maintain the security, privacy, and integrity of PII data and systems. Compliance requires organisations to maintain an auditable trail of personal data through its lifecycle, which includes acquisition, processing, storage, sharing, retention, and disposal of data. It is an ongoing process requiring a blend of people, processes and technology. Tools and technologies play a critical role in the implementation of a governance framework include: \kerning1\expnd0\expndtw0 	a\expnd0\expndtw0\kerning0
uthentication and access control, encryption and data masking, hosting options that comply with requirements and restrictions for international data transfers, monitoring and alerting functionalities, data erasure tools that ensure deleted data cannot be retrieved.\
\
Data ops methodology: is a collaborative data management practice focused on improving the communication, integration, and integration of data flows between data managers and consumers across an organisation. It aims to create predictable delivery and change management of data, data models, and related artefacts. How? through metadata management, workflow and test automation, code repositories, collaboration tools, and orchestration to help manage complex tasks and workflows. Using the DataOps methodology ensures all activities occur in the right order the right security permissions. Purpose? to enable an organisation to utilise a repeatable process to build and deploy analytics and data pipelines. It ensures that the data used in problem-solving and decision making is relevant, reliable, and traceable and improves the probability of achieving desired business outcomes. And it does so by tackling the challenges associated with inefficiencies in accessing, preparing, integrating, and making data available.
\f2\fs32 \cb1 \

\f0\fs48 \cb3 \
Different roles in data engineering\
\
{{\NeXTGraphic 1__#$!@%!#__Pasted Graphic 2.png \width26480 \height12640 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
Data pipeline performance metrics: latency, failures (rate at which a service fails), resource utilisation, traffic (number of requests received in a given period).\cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
NoSQL databases: their advantages and disadvantages, where they are best put to use and different types that exist in the market along with examples in each category.\
\
Overall lesson learned: data engineering is a team sport (especially when working for a larger organisation). Effective communication, team work, and collaboration are essential. \
\
\
Introduction to relational databases course.\
\
What did you learn? Core concepts of relational databases (principles of relational database management). Learn about different database systems and deep dive into how they work; Db2, MySQL (phpMyAdmin), PostgreSQL (pgAdmin). Creating tables, loading data, learning how to design keys, indexes, and constraints. Understand the approach to database design; creating entity relationship diagrams.\
\
Final project requirements:\
\
\pard\pardeftab720\sa320\partightenfactor0
\cf7 \cb3 \expnd0\expndtw0\kerning0
Objectives\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf7 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Identify entities\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Identity attributes\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Create an entity relationship diagram (ERD) using the pgAdmin ERD tool\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Normalize tables\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Define keys and relationships\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Create database objects by generating and running the SQL script from the ERD tool\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Create a view and export the data\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Create a materialised view and export the data\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Import data into a MySQL database using phpMyAdmin GUI tool\
\pard\tx720\pardeftab720\partightenfactor0

\f3\fs28 \cf7 \cb1 \
\
\
\pard\pardeftab720\partightenfactor0

\f0\fs48 \cf2 \cb3 Databases and SQL for Data Science with Python\
\
What did you learn? SQL for data manipulation; DDL statements; built in functions, subqueries and working with multiple tables; use python for accessing databases on Jupyter notebooks (SQL magic, DB-API); stored procedures, view, ACID transactions, inner and outer joins\
\
Data Warehouse Fundamentals\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf6 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Discuss the fundamentals of data warehouses, data marts, and data lakes by analysing their features and advantages.\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Examine organisations' decision-making processes to select specific data warehouse systems and vendors.\'a0\'a0\'a0\'a0\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Design and populate a data warehouse, and
\f1 \uc0\u8239 
\f0 model and query data using Cubes, Rollups, and materialised views.\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Design and load data into a data warehouse, write aggregation queries, create materialised query tables, and create an analytics dashboard.\
\pard\tx720\pardeftab720\partightenfactor0
\cf6 \cb1 \
Final lab objectives:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0\cf7 \cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Design a data warehouse.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Load data into the data warehouse.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Create a materialised view.
\f3\fs28 \cb1 \
\pard\tx720\pardeftab720\partightenfactor0

\f0\fs48 \cf6 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \
Introduction to NoSQL Databases\
\
\pard\pardeftab720\sa320\partightenfactor0
\cf7 \cb1 Module 1: Basics of NoSQL\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf7 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Basics of NoSQL\
\ls4\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Working with Distributed Data\
\pard\pardeftab720\sa320\partightenfactor0
\cf7 Module 2: Introducing MongoDB\'97An Open-Source NoSQL Database\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf7 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Basics of MongoDB\
\ls5\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Getting Started with MongoDB\
\pard\tx720\pardeftab720\partightenfactor0
\cf7 \
\pard\pardeftab720\sa320\partightenfactor0
\cf7 Module 3: Introducing Apache Cassandra\'97An Open-Source NoSQL Database
\f4\b\fs30 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls6\ilvl0
\f3\b0\fs28 \cf7 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}
\f0\fs48 \expnd0\expndtw0\kerning0
Cassandra Basics\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls6\ilvl0\cf7 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Working with Cassandra\
\pard\tx720\pardeftab720\partightenfactor0
\cf7 \
\pard\pardeftab720\sa320\partightenfactor0
\cf7 Final Project Overview\
Successful completion of this project demonstrates your ability to perform the following tasks:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0\cf7 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Import and export data in MongoDB\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Query data in MongoDB\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Import and export data in Cassandra\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Query data in Cassandra
\f3\fs28 \
\pard\tx720\pardeftab720\partightenfactor0
\cf7 \
\pard\pardeftab720\partightenfactor0

\f0\fs48 \cf2 \cb3 \
\pard\pardeftab720\partightenfactor0

\f5\b\fs36 \cf2 \
\pard\pardeftab720\partightenfactor0

\f0\b0\fs48 \cf2 Python for Data Science, AI & Development\
\
What did you learn? Python data structures; working with data in python; APIs and data collection.\
\
Linux Commands and Shell Scripting; more of a review as I was already familiar with a lot of what is taught.
\f5\b\fs36 \

\f0\b0\fs48 \
ETL and Data Pipelines with Shell, Airflow and Kafka\
\
What did you learn? \cf6 fundamental\'a0principles and\'a0techniques behind\'a0ETL and ELT\'a0processes; how to construct a basic ETL\'a0data pipeline\'a0from scratch\'a0using\'a0Bash shell-scripting; the\'a0tools,\'a0technologies, and use\'a0cases\'a0for the\'a0two\'a0main\'a0paradigms\'a0within data pipeline engineering: batch and streaming data pipelines; learn all about Apache Airflow and use it to build, put into production, and monitor a basic batch ETL workflow;  implement data pipeline using Airflow\'92s central construct of a directed acyclic graph (DAG), consisting of Bash tasks, Python function and their dependencies; learn about Apache Kafka and use it to get hands-on experience with streaming data pipelines, implementing Kafka\'92s message producers and consumers, and creating a Kafka weather topic.\
\
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf8 \cb6 from\cf5  datetime \cf8 import\cf5  timedelta\cb1 \
\cf8 \cb6 from\cf5  airflow.models \cf8 import\cf5  DAG\cb1 \
\cf8 \cb6 from\cf5  airflow.operators.python \cf8 import\cf5  PythonOperator\cb1 \
\cf8 \cb6 from\cf5  airflow.utils.dates \cf8 import\cf5  days_ago\cb1 \
\cf8 \cb6 import\cf5  requests\cb1 \
\cf8 \cb6 import\cf5  tarfile\cb1 \
\cf8 \cb6 import\cf5  csv\cb1 \
\
\cf9 \cb6 # Define the path for the input and output files\cf5 \cb1 \
\cb6 source_url \cf10 =\cf5  \cf11 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz'\cf5 \cb1 \
\cb6 destination_path \cf10 =\cf5  \cf11 '/home/project/airflow/dags/python_etl/staging'\cf5 \cb1 \
\
\cf9 \cb6 # Function to download the dataset\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 download_dataset\cf5 ():\cb1 \
\cb6     response \cf10 =\cf5  requests.get(source_url, \cf14 stream\cf10 =\cf12 True\cf5 )\cb1 \
\cb6     \cf8 if\cf5  response.status_code \cf10 ==\cf5  \cf15 200\cf5 :\cb1 \
\cb6         \cf8 with\cf5  \cf13 open\cf5 (\cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /tolldata.tgz"\cf5 , \cf11 'wb'\cf5 ) \cf8 as\cf5  f:\cb1 \
\cb6             f.write(response.raw.read())\cb1 \
\cb6     \cf8 else\cf5 :\cb1 \
\cb6         \cf13 print\cf5 (\cf11 "Failed to download the file"\cf5 )\cb1 \
\
\
\cf9 \cb6 # Function to untar the dataset\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 untar_dataset\cf5 ():\cb1 \
\cb6     \cf8 with\cf5  tarfile.open(\cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /tolldata.tgz"\cf5 , \cf11 "r:gz"\cf5 ) \cf8 as\cf5  tar:\cb1 \
\cb6         tar.extractall(\cf14 path\cf10 =\cf5 destination_path)\cb1 \
\
\
\cf9 \cb6 # Function to extract data from CSV\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 extract_data_from_csv\cf5 ():\cb1 \
\cb6     input_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /vehicle-data.csv"\cf5 \cb1 \
\cb6     output_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /csv_data.csv"\cf5 \cb1 \
\cb6     \cf8 with\cf5  \cf13 open\cf5 (input_file, \cf11 'r'\cf5 ) \cf8 as\cf5  infile, \cf13 open\cf5 (output_file, \cf11 'w'\cf5 ) \cf8 as\cf5  outfile:\cb1 \
\cb6         writer \cf10 =\cf5  csv.writer(outfile)\cb1 \
\cb6         writer.writerow([\cf11 'Rowid'\cf5 , \cf11 'Timestamp'\cf5 , \cf11 'Anonymized Vehicle number'\cf5 , \cf11 'Vehicle type'\cf5 ])\cb1 \
\cb6         \cf8 for\cf5  line \cf8 in\cf5  infile:\cb1 \
\cb6             row \cf10 =\cf5  line.split(\cf11 ','\cf5 )\cb1 \
\cb6             writer.writerow([row[\cf15 0\cf5 ], row[\cf15 1\cf5 ], row[\cf15 2\cf5 ], row[\cf15 3\cf5 ]])\cb1 \
\
\
\cf9 \cb6 # Function to extract data from TSV\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 extract_data_from_tsv\cf5 ():\cb1 \
\cb6     input_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /tollplaza-data.tsv"\cf5 \cb1 \
\cb6     output_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /tsv_data.csv"\cf5 \cb1 \
\cb6     \cf8 with\cf5  \cf13 open\cf5 (input_file, \cf11 'r'\cf5 ) \cf8 as\cf5  infile, \cf13 open\cf5 (output_file, \cf11 'w'\cf5 ) \cf8 as\cf5  outfile:\cb1 \
\cb6         writer \cf10 =\cf5  csv.writer(outfile)\cb1 \
\cb6         writer.writerow([\cf11 'Number of axles'\cf5 , \cf11 'Tollplaza id'\cf5 , \cf11 'Tollplaza code'\cf5 ])\cb1 \
\cb6         \cf8 for\cf5  line \cf8 in\cf5  infile:\cb1 \
\cb6             row \cf10 =\cf5  line.split(\cf11 '\cf16 \\t\cf11 '\cf5 )\cb1 \
\cb6             writer.writerow([row[\cf15 0\cf5 ], row[\cf15 1\cf5 ], row[\cf15 2\cf5 ]])\cb1 \
\
\
\cf9 \cb6 # Function to extract data from fixed width file\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 extract_data_from_fixed_width\cf5 ():\cb1 \
\cb6     input_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /payment-data.txt"\cf5 \cb1 \
\cb6     output_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /fixed_width_data.csv"\cf5 \cb1 \
\cb6     \cf8 with\cf5  \cf13 open\cf5 (input_file, \cf11 'r'\cf5 ) \cf8 as\cf5  infile, \cf13 open\cf5 (output_file, \cf11 'w'\cf5 ) \cf8 as\cf5  outfile:\cb1 \
\cb6         writer \cf10 =\cf5  csv.writer(outfile)\cb1 \
\cb6         writer.writerow([\cf11 'Type of Payment code'\cf5 , \cf11 'Vehicle Code'\cf5 ])\cb1 \
\cb6         \cf8 for\cf5  line \cf8 in\cf5  infile:\cb1 \
\cb6             writer.writerow([line[\cf15 0\cf5 :\cf15 6\cf5 ].strip(), line[\cf15 6\cf5 :\cf15 12\cf5 ].strip()])\cb1 \
\
\
\cf9 \cb6 # Function to consolidate data\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 consolidate_data\cf5 ():\cb1 \
\cb6     csv_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /csv_data.csv"\cf5 \cb1 \
\cb6     tsv_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /tsv_data.csv"\cf5 \cb1 \
\cb6     fixed_width_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /fixed_width_data.csv"\cf5 \cb1 \
\cb6     output_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /extracted_data.csv"\cf5 \cb1 \
\cb6     \cf8 with\cf5  \cf13 open\cf5 (csv_file, \cf11 'r'\cf5 ) \cf8 as\cf5  csv_in, \cf13 open\cf5 (tsv_file, \cf11 'r'\cf5 ) \cf8 as\cf5  tsv_in, \cf13 open\cf5 (fixed_width_file, \cf11 'r'\cf5 ) \cf8 as\cf5  fixed_in, \cf13 open\cf5 (output_file, \cf11 'w'\cf5 ) \cf8 as\cf5  out_file:\cb1 \
\cb6         csv_reader \cf10 =\cf5  csv.reader(csv_in)\cb1 \
\cb6         tsv_reader \cf10 =\cf5  csv.reader(tsv_in)\cb1 \
\cb6         fixed_reader \cf10 =\cf5  csv.reader(fixed_in)\cb1 \
\cb6         writer \cf10 =\cf5  csv.writer(out_file)\cb1 \
\cb6         writer.writerow([\cf11 'Rowid'\cf5 , \cf11 'Timestamp'\cf5 , \cf11 'Anonymized Vehicle number'\cf5 , \cf11 'Vehicle type'\cf5 , \cf11 'Number of axles'\cf5 , \cf11 'Tollplaza id'\cf5 , \cf11 'Tollplaza code'\cf5 , \cf11 'Type of Payment code'\cf5 , \cf11 'Vehicle Code'\cf5 ])\cb1 \
\cb6         \cf13 next\cf5 (csv_reader)\cb1 \
\cb6         \cf13 next\cf5 (tsv_reader)\cb1 \
\cb6         \cf13 next\cf5 (fixed_reader)\cb1 \
\cb6         \cf8 for\cf5  csv_row, tsv_row, fixed_row \cf8 in\cf5  \cf13 zip\cf5 (csv_reader, tsv_reader, fixed_reader):\cb1 \
\cb6             writer.writerow(csv_row \cf10 +\cf5  tsv_row \cf10 +\cf5  fixed_row)\cb1 \
\
\
\cf9 \cb6 # Function to transform data\cf5 \cb1 \
\cf12 \cb6 def\cf5  \cf13 transform_data\cf5 ():\cb1 \
\cb6     input_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /extracted_data.csv"\cf5 \cb1 \
\cb6     output_file \cf10 =\cf5  \cf12 f\cf11 "\cf12 \{\cf5 destination_path\cf12 \}\cf11 /transformed_data.csv"\cf5 \cb1 \
\cb6     \cf8 with\cf5  \cf13 open\cf5 (input_file, \cf11 'r'\cf5 ) \cf8 as\cf5  infile, \cf13 open\cf5 (output_file, \cf11 'w'\cf5 ) \cf8 as\cf5  outfile:\cb1 \
\cb6         reader \cf10 =\cf5  csv.DictReader(infile)\cb1 \
\cb6         writer \cf10 =\cf5  csv.DictWriter(outfile, \cf14 fieldnames\cf10 =\cf5 reader.fieldnames)\cb1 \
\cb6         writer.writeheader()\cb1 \
\cb6         \cf8 for\cf5  row \cf8 in\cf5  reader:\cb1 \
\cb6             row[\cf11 'Vehicle type'\cf5 ] \cf10 =\cf5  row[\cf11 'Vehicle type'\cf5 ].upper()\cb1 \
\cb6             writer.writerow(row)\cb1 \
\
\
\cf9 \cb6 # Default arguments for the DAG\cf5 \cb1 \
\cb6 default_args \cf10 =\cf5  \{\cb1 \
\cb6     \cf11 'owner'\cf5 : \cf11 'Your name'\cf5 ,\cb1 \
\cb6     \cf11 'start_date'\cf5 : days_ago(\cf15 0\cf5 ),\cb1 \
\cb6     \cf11 'email'\cf5 : [\cf11 'xyz@gmail.com'\cf5 ],\cb1 \
\cb6     \cf11 'retries'\cf5 : \cf15 1\cf5 ,\cb1 \
\cb6     \cf11 'retry_delay'\cf5 : timedelta(\cf14 minutes\cf10 =\cf15 5\cf5 ),\cb1 \
\cb6 \}\cb1 \
\
\cf9 \cb6 # Define the DAG\cf5 \cb1 \
\cb6 dag \cf10 =\cf5  DAG(\cb1 \
\cb6     \cf11 'ETL_toll_data'\cf5 ,\cb1 \
\cb6     \cf14 default_args\cf10 =\cf5 default_args,\cb1 \
\cb6     \cf14 description\cf10 =\cf11 'Apache Airflow Final Assignment'\cf5 ,\cb1 \
\cb6     \cf14 schedule_interval\cf10 =\cf5 timedelta(\cf14 days\cf10 =\cf15 1\cf5 ),\cb1 \
\cb6 )\cb1 \
\
\cf9 \cb6 # Define the tasks\cf5 \cb1 \
\cb6 download_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'download_dataset'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 download_dataset,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\cb6 untar_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'untar_dataset'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 untar_dataset,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\cb6 extract_csv_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'extract_data_from_csv'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 extract_data_from_csv,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\cb6 extract_tsv_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'extract_data_from_tsv'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 extract_data_from_tsv,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\cb6 extract_fixed_width_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'extract_data_from_fixed_width'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 extract_data_from_fixed_width,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\cb6 consolidate_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'consolidate_data'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 consolidate_data,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\cb6 transform_task \cf10 =\cf5  PythonOperator(\cb1 \
\cb6     \cf14 task_id\cf10 =\cf11 'transform_data'\cf5 ,\cb1 \
\cb6     \cf14 python_callable\cf10 =\cf5 transform_data,\cb1 \
\cb6     \cf14 dag\cf10 =\cf5 dag,\cb1 \
\cb6 )\cb1 \
\
\cf9 \cb6 # Set the task dependencies\cf5 \cb1 \
\cb6 download_task \cf10 >>\cf5  untar_task \cf10 >>\cf5  [extract_csv_task, extract_tsv_task, extract_fixed_width_task] \cf10 >>\cf5  consolidate_task \cf10 >>\cf5  transform_task\cb1 \
\pard\pardeftab720\partightenfactor0

\f2\fs32 \cf6 \cb3 \
\pard\pardeftab720\partightenfactor0

\f5\b\fs36 \cf2 \
\pard\pardeftab720\partightenfactor0

\f0\b0\fs48 \cf2 Next steps?\
\
Complete the specialisation; machine learning with apache spark, introduction to big data with spark and Hadoop, generative ai, data engineering capstone. \
 
\f5\b\fs36 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b0\fs48 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
}