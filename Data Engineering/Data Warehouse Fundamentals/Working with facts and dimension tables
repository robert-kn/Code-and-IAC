What is the purpose of this lab?

learn how to organise and analyse large datasets using warehousing techniques. These are essential skills for making
informed business decisions, optimising data retrieval, and enhancing understanding of data relationships.

In this lab, I will design a data warehouse for a cloud service provider (CSP). The CSP has given me access to their billing data in the csv file cloud-billing-dataset.csv. The file contains billing data for the past decade. It has the following fields/ columns:

Field Name	    Details
customerid	    Id of the customer
category	    Category of the customer. Example: Individual or Company
country	        Country of the customer 
industry	    Which domain/industry the customer belongs to. Example: Legal, Engineering
month	        The billed month, stored as YYYY-MM. Example: 2009-01 refers to the month January in the year 2009
billedamount	Amount charged by the cloud services provided for that month in USD

I need to design a data warehouse that can support the queries listed below:

average billing per customer
billing by country
top 10 customers
top 10 countries
billing by industry
billing by category
billing by year
billing by month
billing by quarter
average billing per industry per month
average billing per industry per quarter
average billing per country per quarter
average billing per country per industry per quarter

Here are five rows picked at random from the csv file. see csv-rows.png

--------------------------------------------------
Designing the fact table(s) for the star schema
--------------------------------------------------

The fact in this data is the bill which is generated monthly. The fields customerid and billedamount are the important fields in the fact table. I also need to identify the additinal customer information, other than the id, and date information. I will fields that refer to the customer and data information in other tables.

The final fact table for the bill would look somehting like this:

Field Name	        Details
billid	            Primary key - Unique identifier for every bill
customerid	        Foreign Key - Id of the customer
monthid	Foreign     Key - Id of the month. We can resolve the billed month info using this
billedamount	    Amount charged by the cloud services provided for that month in USD

There are two dimensions to our fact(monthly bill)

1. customer inforamtion
2. date information

the below fields give information about the customer in a dimension table

Field Name	Details
customerid	Primary Key - Id of the customer
category	Category of the customer. Example: Individual or Company
country	    Country of the customer
industry	Which domain/industry the customer belongs to. Example: Legal, Engineering

the below fields give information about the date of the bill

Field Name	Details
monthid	    Primary Key - Id of the month
year	    Year derived from the month field of the original data. Example: 2010
month	    Month number derived from the month field of the original data. Example: 1, 2, 3
monthname	Month name derived from the month field of the original data. Example: March
quarter	    Quarter number derived from the month field of the original data. Example: 1, 2, 3, 4
quartername	Quarter name derived from the month field of the original data. Example: Q1, Q2, Q3, Q4


Based on the above, I now have 3 tables which are identified below

Table Name	    Type	    Details
FactBilling	    Fact	    This table contains the billing amount, and the foreign keys to customer and month data
DimCustomer	    Dimension	This table contains all the information related the customer
DimMonth	    Dimension	This table contains all the information related the month of billing

Arranging the above tables in star schema style I arrive at a table structure that looks as follows see star-diagram.png

The image shows the fact and dimension tables along with the relationships between them.

------------------------------------------
Creating the schema on the data warehouse
------------------------------------------

this will be done on postgresql server

Firstly, run the command below to set your PostgreSQL password for authentication. Replace <your_password> with your actual PostgreSQL password, and then execute the command:

export PGPASSWORD=<your_password>

Now, run the command below to create a database named billingDW.

createdb -h postgres -U postgres -p 5432 billingDW

In the above command, -h mentions that the database server is accessible using the hostname “postgres”, -U mentions that we are using the user name postgres to log into the database, -p mentions that the database server is running on port number 5432

Download the schema .sql file.

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0260EN-SkillsNetwork/labs/Working%20with%20Facts%20and%20Dimension%20Tables/star-schema.sql

the above schema is as follows

BEGIN;


CREATE TABLE public."FactBilling"
(
    rowid integer NOT NULL,
    customerid integer NOT NULL,
    monthid integer NOT NULL,
    billedamount integer NOT NULL,
    PRIMARY KEY (rowid)
);

CREATE TABLE public."DimMonth"
(
    monthid integer NOT NULL,
    year integer NOT NULL,
    month integer NOT NULL,
    monthname "char" NOT NULL,
    quarter integer NOT NULL,
    quartername "char" NOT NULL,
    PRIMARY KEY (monthid)
);

CREATE TABLE public."DimCustomer"
(
    customerid integer NOT NULL,
    category "char" NOT NULL,
    country "char" NOT NULL,
    industry "char" NOT NULL,
    PRIMARY KEY (customerid)
);

ALTER TABLE public."FactBilling"
    ADD FOREIGN KEY (customerid)
    REFERENCES public."DimCustomer" (customerid)
    NOT VALID;


ALTER TABLE public."FactBilling"
    ADD FOREIGN KEY (monthid)
    REFERENCES public."DimMonth" (monthid)
    NOT VALID;

END;

Run the command below to create the schema in the under billingDW database.

psql  -h postgres -U postgres -p 5432 billingDW < star-schema.sql




Practice exercise

In this practice exercise, you will analyze the below csv file, which contains data about the daily sales at different stores of an international fashion retailer. see sales-csv.png

Design the schema for the fact table FactSales.

Field Name	Details
rowid	    Primary key - Unique identifier for every row
storeid	    Foreign Key - Id of the store
dateid	    Foreign Key - Id of the date
totalsales	Total sales


Design the schema for the dimension table DimStore.

Field Name	Details
storeid	    Primary key - Unique identifier for every store
city	    City where the store is located.
country	    Country where the store is located.


Design the schema for the dimension table DimDate.

Field Name	    Details
dateid	        Primary Key - Id of the date
day	            Day derived from the date field of the original data. Example: 13, 19
weekday	        Weekday derived from the date field of the original data. Example: 1, 2, 3, 4, 5, 6, 7. 1 for sunday, 7 for saturday
weekdayname	    Weekday name derived from the date field of the original data. Example: Sunday, Monday
year	        Year derived from the date field of the original data. Example: 2010
month	        Month number derived from the date field of the original data. Example: 1, 2, 3
monthname	    Month name derived from the date field of the original data. Example: March
quarter	        Quarter number derived from the date field of the original data. Example: 1, 2, 3, 4
quartername	    Quarter name derived from the date field of the original data. Example: Q1, Q2, Q3, Q4


---------------------------------
Setting up a staging area
---------------------------------

Create Database

Using the createdb command of the PostgreSQL server, we can directly create the database from the terminal.

Before that, export your password

export PGPASSWORD=<your_password>;

Then run the below command which will create a database named billingDW.

createdb -h postgres -U postgres -p 5432 billingDW

In the above command

-h mentions that the database server is accessible using the hostname “postgres”
-U mentions that we are using the user name postgres to log into the database
-p mentions that the database server is running on port number 5432

Create data warehouse schema 

Run the commands below to download and extract the schema files.

1. wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0260EN-SkillsNetwork/labs/Setting%20up%20a%20staging%20area/billing-datawarehouse.tgz
2. tar -xvzf billing-datawarehouse.tgz

Run the command below to create the schema in the billingDW database.

psql  -h postgres -U postgres -p 5432 billingDW < star-schema.sql

When we load data into the tables, it is a good practice to load the data into dimension tables first.

Load data into DimCustomer table

Run the command below to load the data into DimCustomer table in billingDW database.

psql  -h postgres -U postgres -p 5432 billingDW < DimCustomer.sql

Load data into DimMonth table

Run the command below to load the data into DimMonth table in billingDW database.

psql  -h postgres -U postgres -p 5432 billingDW < DimMonth.sql

Load data into FactBilling table

Run the command below to load the data into FactBilling table in billingDW database.

psql  -h postgres -U postgres -p 5432 billingDW < FactBilling.sql

Run the command below to check the number of rows in all the tables in the billingDW database.

psql  -h postgres -U postgres -p 5432 billingDW < verify.sql

----------------------
END OF LAB
----------------------

what is data quality verification? see dataVerification1.png dataVerification2.png dataVerification3.png dataVerification4.png

Expanding on data quality concerns? see dataQuality1.png dataQuality2.png dataQuality3.png dataQuality4.png dataQuality5.png dataQuality6.png dataQuality7.png dataQuality8.png dataQuality9.png dataQuality10.png
dataQuality11.png dataQuality12.png

--------------------------------------------------------------------------------------------------------------------
The primary purpose of this lab is to instruct participants on the process of conducting thorough data quality checks in a data warehousing environment. It focuses on using a Python-based framework within a PostgreSQL database to validate data integrity. Key areas of emphasis include identifying null values, duplicates, and invalid entries, as well as verifying data ranges. The lab aims to equip learners with the necessary skills to set up and utilize a testing framework for data validation, ensuring data accuracy and consistency.  
--------------------------------------------------------------------------------------------------------------------

In this lab, you will:

Check Null values
Check Duplicate values
Check Min Max
Check Invalid values
Generate a report on data quality

Run the command below to download the staging area setup script.

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0260EN-SkillsNetwork/labs/Verifying%20Data%20Quality%20for%20a%20Data%20Warehouse/setup_staging_area.sh

Run the setup script.

Run the command below to execute the staging area setup script.

bash setup_staging_area.sh (before running the script, first export the password of the database instance as a shell variable and then replace localhost in the script with postgres which is the name of the database server)

Getting the testing framework ready

You can perform most of the data quality checks by manually running sql queries on the data warehouse.

It is a good idea to automate these checks using custom programs or tools. Automation helps you to easily

create new tests,
run tests,
and schedule tests.
We will be using a python based framework to run the data quality tests.

Step 1: Download the framework.

Run the commands below to download the framework

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0260EN-SkillsNetwork/labs/Verifying%20Data%20Quality%20for%20a%20Data%20Warehouse/dataqualitychecks.py

Install the python driver for Postgresql.

Run the command below to install the python driver for Postgresql database

python3 -m pip install psycopg2

Update Password in dbconnect.py

Once done, save the file.

Test database connectivity.

Now we need to check

if the Postgresql python driver is installed properly.
if Postgresql server is up and running.
if our micro framework can connect to the database.
The command below to check all the above cases.

python3 dbconnect.py

If all goes well, you should a message Successfully connected to database.

The command also disconnects from the server with a message Connection closed.

Create a sample data quality report

Run the command below to install pandas.

python3 -m pip install pandas tabulate

Run the command below to generate a sample data quality report.

python3 generate-data-quality-report.py

You should see a list of tests that were run and their status.

Explore the data quality tests

Open the file mytests.py in the editor

The file mytests.py contains all the data quality tests.

It provides a quick and easy way to author and run new data quality tests.

The testing framework provides the following tests:

check_for_nulls - this test will check for nulls in a column
check_for_min_max - this test will check if the values in a column are with a range of min and max values
check_for_valid_values - this test will check for any invalid values in a column
check_for_duplicates - this test will check for duplicates in a column
Each test can be authored by mentioning a minimum of 4 parameters.

testname - The human readable name of the test for reporting purposes
test - The actual test name that the testing micro framework provides
table - The table name on which the test is to be performed
column - The table name on which the test is to be performed

Let us now see what a check_for_nulls test looks like.

Here is a sample check_for_nulls test:

test1={
    "testname":"Check for nulls",
    "test":check_for_nulls,
    "column": "monthid",
    "table": "DimMonth"
}

All tests must be named as test following by a unique number to identify the test.

Give an easy to understand description for testname
mention check_for_nulls for test
mention the column name on which you wish to check for nulls
mention the table name where this column exists
Let us now create a new check_for_nulls test and run it.

The test below checks if there are any null values in the column year in the table DimMonth.

The test fails if nulls exist.

Copy and paste the code below at the end of mytests.py file.

Save the file using Menu -> File -> Save

Run the command below to generate the new data quality report.

python3 generate-data-quality-report.py

Let us now see what a check_for_min_max test looks like.

Here is a sample check_for_min_max test

test2={
    "testname":"Check for min and max",
    "test":check_for_min_max,
    "column": "monthid",
    "table": "DimMonth",
    "minimum":1,
    "maximum":12
}

In addition to the usual fields, you have two more fields here.

minimum is the lowest valid value for this column. (Example 1 in case of month number)
maximum is the highest valid value for this column. (Example 12 in case of month number)
Let us now create a new check_for_min_max test and run it.

The test below checks for minimum of 1 and maximum of 4 in the column quarter in the table DimMonth.

The test fails if there any values less than minimum or more than maximum.

Copy and paste the code below at the end of mytests.py file.

test6={
    "testname":"Check for min and max",
    "test":check_for_min_max,
    "column": "quarter",
    "table": "DimMonth",
    "minimum":1,
    "maximum":4
}

Save the file 

Run the command below to generate the new data quality report.

python3 generate-data-quality-report.py

Let us now see what a check_for_valid_values test looks like.

Here is a sample check_for_valid_values test:

test3={
    "testname":"Check for valid values",
    "test":check_for_valid_values,
    "column": "category",
    "table": "DimCustomer",
    "valid_values":{'Individual','Company'}
}

In addition to the usual fields, you have an additional field here.

use the field valid_values to mention what are the valid values for this column.
Let us now create a new check_for_valid_values test and run it.

The test below checks for valid values in the column quartername in the table DimMonth.

The valid values are Q1,Q2,Q3,Q4

The test fails if there any values less than minimum or more than maximum.

Copy and paste the code below at the end of mytests.py file.

Save the file

Run the command below to generate the new data quality report.

python3 generate-data-quality-report.py

Check for duplicate entries
Let us now see what a check_for_duplicates test looks like.

Here is a sample check_for_duplicates test

test4={
    "testname":"Check for duplicates",
    "test":check_for_duplicates,
    "column": "monthid",
    "table": "DimMonth"
}

Let us now create a new check_for_duplicates test and run it.

The test below checks for any duplicate values in the column customerid in the table DimCustomer.

The test fails if duplicates exist.

Copy and paste the code below at the end of mytests.py file.

test8={
    "testname":"Check for duplicates",
    "test":check_for_duplicates,
    "column": "customerid",
    "table": "DimCustomer"
}

Save the file

Run the command below to generate the new data quality report.

python3 generate-data-quality-report.py

Practice exercises
Problem:
Create a check_for_nulls test on column billedamount in the table FactBilling


Problem:
Create a check_for_duplicates test on column billid in the table FactBilling


Problem:
Create a check_for_valid_values test on column quarter in the table DimMonth. The valid values are 1, 2, 3, 4

-------------------------------
end of lab
-------------------------------

populating a data warehouse populatingDataWarehouse1.png populatingDataWarehouse2.png populatingDataWarehouse3.png populatingDataWarehouse4.png populatingDataWarehouse5.png populatingDataWarehouse6.png populatingDataWarehouse7.png populatingDataWarehouse8.png populatingDataWarehouse9.png populatingDataWarehouse10.png populatingDataWarehouse11.png populatingDataWarehouse12.png populatingDataWarehouse13.png populatingDataWarehouse14.png populatingDataWarehouse15.png 


-------------------------------------------------------------------------------------------------------------------
The lab is designed to provide hands-on experience in creating and managing a production database using PostgreSQL within the IBM Skills Network Labs (SN Labs) Cloud IDE. You will learn how to launch a PostgreSQL server instance, utilize the pgAdmin graphical user interface (GUI) for database operations, and execute essential tasks like creating a database, designing tables, and loading data. The lab focuses on building a foundation in database management by guiding learners through the process of setting up a 'Production' database and populating it with data following a star schema design.

Engaging in this lab offers significant benefits for learners seeking to deepen their understanding of database management systems, particularly PostgreSQL. By working through the lab, you will gain practical skills in SQL, database creation, table design, and data manipulation, which are crucial for roles in data engineering, database administration, and data science. The hands-on approach helps in consolidating knowledge of database schemas and SQL queries, thereby enhancing the learner's ability to manage and analyze data effectively in real-world scenarios. Additionally, familiarity with tools like pgAdmin and the Cloud IDE environment adds valuable experience to your skill set, preparing you for advanced database projects and tasks.
-------------------------------------------------------------------------------------------------------------------

Production database is used in this lab.
The production database contains:

DimCustomer
DimMonth
FactBilling
Objectives
In this lab you will:

Create production related database and tables in a PostgreSQL instance.
Populate the production data warehouse byloading the tables from Scripts.

The following are the SQL data files used in this lab.

The production database contains (find them in folder):

DimCustomer
DimMonth
FactBilling
Star Schema

Create a database

First, to create a database on a PostgreSQL server instance, you'll first want to actually launch a PostgreSQL server instance.

Once the pgAdmin GUI opens, click on the Servers tab on the left side of the page. You will be prompted to enter a password. see pgAdmin0.png pgAdmin1.png

Retrieve your password, and paste it in the UI to access the pgAdmin GUI tool.

In the left tree-view, right-click on Databases> Create > Database. see create1.png

In the Database box, type Production as the name for your new database, and then click Save. Proceed to Task B. create2.png

Now, that you have your PostgreSQL service active and have created the Production database using pgAdmin, let's go ahead and create a few tables to populate the database and store the data that we wish to eventually upload into it.

Click on the Production database and in the top of the page go to Query tool and then click on Open File. Next a new page pops up called Select File. Click on Upload icon as shown in the screenshot. see addSchemas1.png

Note: Ensure that you upload the files to this path: /var/lib/pgadmin/

In the new blank page that appears drag and drop the star-schema.sql, DimCustomer.sql, DimMonth.sql, and FactBilling.sql.

Once all the schema files are successfully loaded, click on the X icon on the right hand side of the page as shown in the screenshot. addSchemas2.png

Once you click on the X icon a new page appears showing all schemas. Select the each schema in turn and click the run command as shown in the screenshot. see runSchema1.png runSchema2.png

Next, right-click on the Production database and click on the Refresh option from the dropdown. see refresh.png

After the database is refreshed the 3 tables (DimCustomer, DimMonth,FactBilling) are created under the Databases > Production > Schemas > Public > Tables. see refresh.png

Let's run the command below on the PostgreSQL Tool.

select count(*) from public."DimMonth";

You should see an output as see example1.png

Using the PostgreSQL tool, find the count of rows in the table FactBilling

select count(*) from public."FactBilling";

Using the PostgreSQL tool, create a simple Materialized views named avg_customer_bill with fields customerid and averagebillamount.

CREATE MATERIALIZED VIEW  avg_customer_bill (customerid, averagebillamount) AS
(select customerid, avg(billedamount)
from public."FactBilling"
group by customerid
);

Refresh the newly created Materialized views

REFRESH MATERIALIZED VIEW avg_customer_bill;

Using the newly created Materialized views find the customers whose average billing is more than 11000.

select * from avg_customer_bill where averagebillamount > 11000;

---------------------------------
end of lab
---------------------------------

see queryingData.png


-----------------------------------------------------------------------------------------------------
The purpose of this lab is to provide hands-on experience in advanced SQL query techniques using PostgreSQL in a Cloud IDE environment. The lab focuses on teaching how to create grouping sets, rollups, and cubes for data aggregation and summarization, as well as how to implement and utilize Materialized Query Tables (Materialized views) for efficient data querying. These skills are essential for managing and analyzing large datasets, particularly in data warehousing and business intelligence contexts.

By completing this lab, you will gain valuable insights into the practical application of complex SQL queries and data manipulation techniques. The knowledge of grouping sets, rollups, and cubes will enable learners to effectively summarize and analyze data, which is crucial in making informed business decisions. Understanding and implementing Materialized views provides an efficient way to handle large-scale data by reducing the computational load during frequent query executions. These skills are highly beneficial for careers in data analysis, database administration, and business intelligence, enhancing your ability to manage and analyze data in real-world scenarios.
----------------------------------------------------------------------------------------------------

In this lab you will learn how to create:

Grouping sets
Rollup
Cube
Materialized views

This lab requires that you complete the previous lab Populate a Data Warehouse.

If you have not finished the Populate a Data Warehouse Lab yet, please finish it before you continue.

GROUPING SETS, CUBE, and ROLLUP allow us to easily create subtotals and grand totals in a variety of ways. All these operators are used along with the GROUP BY operator.

GROUPING SETS operator allows us to group data in a number of different ways in a single SELECT statement.

The ROLLUP operator is used to create subtotals and grand totals for a set of columns. The summarized totals are created based on the columns passed to the ROLLUP operator.

The CUBE operator produces subtotals and grand totals. In addition, it produces subtotals and grand totals for every permutation of the columns provided to the CUBE operator.

After you launch a PostgreSQL server instance on Cloud IDE and open up the pgAdmin Graphical User Interface run the below query.

To create a grouping set for three columns labeled year, category, and sum of billedamount, run the sql statement below.

select year,category, sum(billedamount) as totalbilledamount
from "FactBilling"
left join "DimCustomer"
on "FactBilling".customerid = "DimCustomer".customerid
left join "DimMonth"
on "FactBilling".monthid="DimMonth".monthid
group by grouping sets(year,category);

To create a rollup using the three columns year, category and sum of billedamount, run the sql statement below.

select year,category, sum(billedamount) as totalbilledamount
from "FactBilling"
left join "DimCustomer"
on "FactBilling".customerid = "DimCustomer".customerid
left join "DimMonth"
on "FactBilling".monthid="DimMonth".monthid
group by rollup(year,category)
order by year, category;

To create a cube using the three columns labeled year, category, and sum of billedamount, run the sql statement below.

select year,category, sum(billedamount) as totalbilledamount
from "FactBilling"
left join "DimCustomer"
on "FactBilling".customerid = "DimCustomer".customerid
left join "DimMonth"
on "FactBilling".monthid="DimMonth".monthid
group by cube(year,category)
order by year, category;

Create a Materialized Query Table(Materialized views)

In pgAdmin we can implement materialized views using Materialized Query Tables.

Step 1: Create the Materialized views.
Execute the sql statement below to create an Materialized views named countrystats.

CREATE MATERIALIZED VIEW countrystats (country, year, totalbilledamount) AS
(select country, year, sum(billedamount)
from "FactBilling"
left join "DimCustomer"
on "FactBilling".customerid = "DimCustomer".customerid
left join "DimMonth"
on "FactBilling".monthid="DimMonth".monthid
group by country,year);

The above command creates an Materialized views named countrystats that has 3 columns.

Country
Year
totalbilledamount

The Materialized views is essentially the result of the below query, which gives you the year, quartername and the sum of billed amount grouped by year and quartername.


select year, quartername, sum(billedamount) as totalbilledamount
from "FactBilling"
left join "DimCustomer"
on "FactBilling".customerid = "DimCustomer".customerid
left join "DimMonth"
on "FactBilling".monthid="DimMonth".monthid
group by grouping sets(year, quartername);

Step 2: Populate/refresh data into the Materialized views.
Execute the sql statement below to populate the Materialized views countrystats.

REFRESH MATERIALIZED VIEW countrystats;

The command above populates the Materialized views with relevant data.

Step 3: Query the Materialized views.

Once an Materialized views is refreshed, you can query it.

Execute the sql statement below to query the Materialized views countrystats.

select * from countrystats;

Problem 1: Create a grouping set for the columns year, quartername, sum(billedamount).

select year, quartername, sum(billedamount) as totalbilledamount
from "FactBilling"
left join "DimCustomer"
on "FactBilling".customerid = "DimCustomer".customerid
left join "DimMonth"
on "FactBilling".monthid="DimMonth".monthid
group by grouping sets(year, quartername);


Problem 2: Create a rollup for the columns country, category, sum(billedamount).

SELECT country, category, SUM(billedamount) AS totalbilledamount
FROM "FactBilling"
LEFT JOIN "DimCustomer"
ON "FactBilling".customerid = "DimCustomer".customerid
LEFT JOIN "DimMonth"
ON "FactBilling".monthid = "DimMonth".monthid
GROUP BY ROLLUP(country, category)
ORDER BY country, category;


Problem 3: Create a cube for the columns year,country, category, sum(billedamount).

SELECT year, country, category, SUM(billedamount) AS totalbilledamount
FROM "FactBilling"
LEFT JOIN "DimCustomer" 
ON "FactBilling".customerid = "DimCustomer".customerid
LEFT JOIN "DimMonth" 
ON "FactBilling".monthid = "DimMonth".monthid
GROUP BY CUBE(year, country, category);


Problem 4: Create an Materialized views named average_billamount with columns year, quarter, category, country, average_bill_amount.

CREATE MATERIALIZED VIEW average_billamount (year,quarter,category,country, average_bill_amount) AS
    (select   year,quarter,category,country, avg(billedamount) as average_bill_amount
    from "FactBilling"
    left join  "DimCustomer"
    on "FactBilling".customerid =  "DimCustomer".customerid
    left join "DimMonth"
    on "FactBilling".monthid="DimMonth".monthid
    group by year,quarter,category,country
    );

refresh MATERIALIZED VIEW average_billamount;

---------------------------------
end of lab
---------------------------------